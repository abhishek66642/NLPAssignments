{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpr5hcWCSgl9"
      },
      "source": [
        "\"\"\"\n",
        "TODO: Your code below!\n",
        "\n",
        "This file should implement all steps described in Part 2, and can be structured however you want.\n",
        "\n",
        "Rather than using normal BERT, you should use distilbert-base-uncased. This will train faster.\n",
        "\n",
        "We recommend training on a GPU, either by using HPC or running the command line commands on Colab.\n",
        "\n",
        "Hints:\n",
        "    * It will probably be helpful to save intermediate outputs (preprocessed data).\n",
        "    * To save your finetuned models, you can use torch.save().\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj11DyE3Seo0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wZuGQ5FI_am",
        "outputId": "e0d15232-aebc-4c8c-b643-d8cc2bc0ccff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-09 03:06:12.520190: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.8/dist-packages (4.5.2)\n"
          ]
        }
      ],
      "source": [
        "# Downloading Dependencies\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install conllu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTSG4KmJWDCw",
        "outputId": "3d595d5f-7c1d-40f1-88e0-f38ff6b0f573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/My Drive/NLPAssignments/hw3\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd drive/My Drive/NLPAssignments/hw3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oP6kA4tUjgq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from src.dependency_parse import DependencyParse\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwm1tuirUYck"
      },
      "outputs": [],
      "source": [
        "current_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wacdhyN0CH8V"
      },
      "outputs": [],
      "source": [
        "# save index 0 for unk and 1 for pad\n",
        "global PAD_IDX, UNK_IDX, CLS_IDX, SEP_IDX\n",
        "PAD_IDX = 0\n",
        "CLS_IDX = 1\n",
        "SEP_IDX = 2\n",
        "UNK_IDX = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lccZaMW9KwxS"
      },
      "outputs": [],
      "source": [
        "def build_vocab(text_tokens, max_vocab_size=10000):\n",
        "    # TODO:\n",
        "    # build vocab\n",
        "    # returns: \n",
        "    # - id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
        "    # - token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
        "\n",
        "    all_tokens = [text_token for tokens in text_tokens for text_token in tokens]\n",
        "\n",
        "    token_count = Counter(all_tokens)\n",
        "    vocab,count = zip(*token_count.most_common(max_vocab_size))\n",
        "    id2token = [\"<PAD>\",\"<CLS>\",\"<SEP>\",\"<UNK>\"] + list(vocab)\n",
        "    token2id = dict(zip(vocab, range(2, 2 + len(vocab)))) # Getting tokens and their indices\n",
        "    token2id[\"<PAD>\"] = PAD_IDX\n",
        "    token2id[\"<CLS>\"] = CLS_IDX\n",
        "    token2id[\"<SEP>\"] = SEP_IDX\n",
        "    token2id[\"<UNK>\"] = UNK_IDX\n",
        "\n",
        "    return token2id,id2token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xt5dV0tj3rC"
      },
      "outputs": [],
      "source": [
        "def token2index(tokens_data,token2id):\n",
        "    # TODO:\n",
        "    # convert token to id in the dataset\n",
        "    # returns:\n",
        "    # - indices_data: return list of index_list (index list for each sentence)\n",
        "    indices_data = []\n",
        "    for tokens in tqdm(tokens_data):\n",
        "      indices = [token2id.get(token, 1) for token in tokens]\n",
        "      indices_data.append(indices)\n",
        "    return indices_data\n",
        "\n",
        "def encode_data(token_list,token2id):\n",
        "  encoded_list = []\n",
        "  for tokens in token_list:\n",
        "    token_indices = []\n",
        "    for token in tokens:\n",
        "      token_indices.append(token2id.get(token))\n",
        "    encoded_list.append(token_indices)\n",
        "  return encoded_list\n",
        "\n",
        "def encode_rel_pos(rel_pos_list,rel_pos_vocab):\n",
        "  rel_pos_idx_list = []\n",
        "  \n",
        "  for rel_pos in rel_pos_list:\n",
        "    rel_idx = []\n",
        "    for rel in rel_pos:\n",
        "      rel_idx.append(rel_pos_vocab.index(rel))\n",
        "    rel_pos_idx_list.append(rel_idx)\n",
        "  \n",
        "  return rel_pos_idx_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYs1I7H6TrTo",
        "outputId": "0d110d22-6275-43d2-fe75-fd7d8a239ab5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/en_gum/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n",
            "WARNING:datasets.builder:Found cached dataset universal_dependencies (/root/.cache/huggingface/datasets/universal_dependencies/en_gum/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative position Vocab Size is 122\n",
            "Relative position Vocab Size is 122\n"
          ]
        }
      ],
      "source": [
        "def make_data(ud_dataset):\n",
        "  dependency_parse_list = []\n",
        "  iterable = iter(ud_dataset_train)\n",
        "  text_list = []\n",
        "  rel_list = []\n",
        "  deprel_list = []\n",
        "  rel_pos_vocab = []\n",
        "  dep_pos_vocab = []\n",
        "  token_list = []\n",
        "\n",
        "  for ctr in range(ud_dataset_train.num_rows):\n",
        "    dependency_parse_obj = DependencyParse.from_huggingface_dict(next(iterable))\n",
        "    text_list.append(dependency_parse_obj.text)\n",
        "    token_list.append(dependency_parse_obj.tokens)\n",
        "    token_indices_list = list(enumerate(dependency_parse_obj.tokens))\n",
        "    head_list = dependency_parse_obj.heads\n",
        "    rel_position_list = [int(head_list[i])-token_indices_list[i][0]-1 if (dependency_parse_obj.deprel[i]!='root') else 0 for i in range(len(token_indices_list))]\n",
        "    rel_pos_vocab.extend(rel_position_list)\n",
        "    rel_list.append(rel_position_list) # Adding list of relative position labels\n",
        "    deprel_list.append(dependency_parse_obj.deprel) # Adding list of dependency labels\n",
        "    dep_pos_vocab.extend(dependency_parse_obj.deprel)\n",
        "\n",
        "  dependency_df = pd.DataFrame({'text':text_list,'rel_pos':rel_list,'dep_label':deprel_list})\n",
        "  rel_pos_vocab = list(set(rel_pos_vocab))\n",
        "  rel_pos_vocab.insert(0,\"[UNK]\")\n",
        "  rel_pos_vocab.insert(0,\"[PAD]\")\n",
        "  print (\"Relative position Vocab Size is \"+str(len(rel_pos_vocab)))\n",
        "  return dependency_df,rel_pos_vocab,dep_pos_vocab,deprel_list,token_list\n",
        "  #dependency_df.to_csv('en_gum_10.tsv',sep='\\t',index=False)\n",
        "\n",
        "ud_dataset_train = load_dataset(\"universal_dependencies\",\"en_gum\", split=\"train\")\n",
        "ud_dataset_valid = load_dataset(\"universal_dependencies\",\"en_gum\", split=\"validation\")\n",
        "train_df,rel_pos_vocab_train,dep_pos_vocab_train,deprel_list_train,token_list_train = make_data(ud_dataset_train)\n",
        "valid_df,rel_pos_vocab_valid,dep_pos_vocab_valid,deprel_list_valid,token_list_valid = make_data(ud_dataset_valid)\n",
        "\n",
        "ud_data = {\"train\":train_df,\"valid\":valid_df}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NLCshBLFf9C",
        "outputId": "d7996d0f-cb85-4bc5-b7c0-43b5014f0de8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4287/4287 [00:00<00:00, 81319.59it/s]\n",
            "100%|██████████| 81861/81861 [00:00<00:00, 95346.40it/s]\n"
          ]
        }
      ],
      "source": [
        "token2id, id2token = build_vocab(token_list_train, max_vocab_size=10000)\n",
        "text_token_train = token2index(token_list_train,token2id)\n",
        "token2iddep, id2tokendep = build_vocab(deprel_list_train, max_vocab_size=10000)\n",
        "text_dep_train = token2index(dep_pos_vocab_train,token2iddep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc0ynHT0RUZq"
      },
      "outputs": [],
      "source": [
        "class FineTuneDataset(Dataset):\n",
        "  def __init__(self, text_list, dep_label_list ,rel_pos_list):\n",
        "        \"\"\"\n",
        "        @param text_list: list of text tokens\n",
        "        @param dep_label_list: list of dependency labels\n",
        "        @param rel_pos_list: list of relative positions\n",
        "        \"\"\"\n",
        "        self.text_list = text_list\n",
        "        self.dep_tensors = []\n",
        "        self.rel_tensors = []\n",
        "        for sample in dep_label_list:\n",
        "          self.dep_tensors.append(torch.tensor(encode_data([sample],token2iddep), dtype=torch.long).to(current_device)) # Adding idx of dependency vocab to Dataset\n",
        "        for sample in rel_pos_list:\n",
        "          self.rel_tensors.append(torch.tensor(encode_rel_pos([sample],rel_pos_vocab_train), dtype=torch.long).to(current_device)) # Adding indices of rel_pos_vocab to Dataset\n",
        "  def __len__(self):\n",
        "    return len(self.text_list)\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    token_text_idx = self.text_list[key]\n",
        "    return [token_text_idx,self.dep_tensors[key], self.rel_tensors[key]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74iVsc2-t8cY"
      },
      "outputs": [],
      "source": [
        "ud_train = FineTuneDataset(list(ud_data['train']['text']),list(ud_data['train']['dep_label']),list(ud_data['train']['rel_pos']))\n",
        "ud_valid = FineTuneDataset(list(ud_data['valid']['text']),list(ud_data['valid']['dep_label']),list(ud_data['valid']['rel_pos']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25A699dKzvo-"
      },
      "outputs": [],
      "source": [
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    text_list = [s[0] for s in batch]\n",
        "    dep_list = [s[1] for s in batch]\n",
        "    rel_list = [s[2] for s in batch]\n",
        "    \n",
        "    return [text_list,dep_list, rel_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plZFV7UInqEb"
      },
      "outputs": [],
      "source": [
        "ud_train_generator = DataLoader(ud_train, batch_size=32,shuffle=True,collate_fn=pad_collate_fn)\n",
        "ud_valid_generator = DataLoader(ud_valid, batch_size=32,shuffle=True,collate_fn=pad_collate_fn)\n",
        "ud_generator = {\"train\":ud_train_generator,\"valid\":ud_valid_generator}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl8E6cwOxKFk"
      },
      "outputs": [],
      "source": [
        "class DependencyParseBertModel(nn.Module):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  def __init__(self,options):\n",
        "    super().__init__()\n",
        "\n",
        "    self.distil_bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    self.projection_rel = nn.Linear(options['d_hidden'],options['rel_vocab'])\n",
        "    self.projection_dep = nn.Linear(options['d_hidden'],options['dep_vocab'])\n",
        "\n",
        "  def forward(self,encoded_input_tensor):\n",
        "    output, = self.distil_bert_model(**encoded_input_tensor).values() # [batch_size,seq_len,768] \n",
        "    rel_dep_logits = self.projection_rel(output) # [batch_size,121]\n",
        "    dep_label_logits = self.projection_dep(output) # [batch_size,48]\n",
        "    return rel_dep_logits,dep_label_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPGAKbjCdHqo",
        "outputId": "bb9ad456-1160-4fc0-97d1-6cc5e31b28e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "options = {\n",
        "    'rel_vocab':122,\n",
        "    'dep_vocab':48,\n",
        "    'd_hidden':768\n",
        "    }\n",
        "\n",
        "model = DependencyParseBertModel(options).to(current_device)\n",
        "\n",
        "rel_criterion = nn.CrossEntropyLoss(ignore_index=0).to(current_device)\n",
        "dep_criterion = nn.CrossEntropyLoss(ignore_index=0).to(current_device)\n",
        "\n",
        "params_to_update = []\n",
        "for name,param in model.named_parameters():\n",
        "  if param.requires_grad == True:\n",
        "    params_to_update.append(param)\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=2e-05, eps=1e-08)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHaxxHyykohI"
      },
      "outputs": [],
      "source": [
        "def custom_tokenize(bert_tokenized_tokens):\n",
        "  bert_tokenize_clean_idx = []\n",
        "  for word_id in range(len(bert_tokenized_tokens)):\n",
        "    if (bert_tokenized_tokens[word_id].startswith(\"##\") or bert_tokenized_tokens[word_id]==\"-\" or (word_id!=0 and bert_tokenized_tokens[word_id-1]==\"-\")):\n",
        "        continue\n",
        "    bert_tokenize_clean_idx.append(word_id)\n",
        "  \n",
        "  return bert_tokenize_clean_idx\n",
        "\n",
        "def pad_list_of_tensors(list_of_tensors, pad_token,max_len):\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t.to(current_device), torch.tensor([[pad_token]*(max_len - t.size(-1))], dtype=torch.long).to(current_device)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.stack(padded_list, dim=1).squeeze(0).to(current_device)\n",
        "    \n",
        "    return padded_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqeOOiYF3Y9S",
        "outputId": "417987e3-0f96-46f3-f200-2fdc13f10eaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.80020809173584\n",
            "Dependecy Label Loss 3.876997709274292\n",
            "Overall Loss 4.107800483703613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r1it [00:12, 12.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.814797878265381\n",
            "Dependecy Label Loss 3.881988525390625\n",
            "Overall Loss 4.1151909828186035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r2it [00:23, 11.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.802700519561768\n",
            "Dependecy Label Loss 3.87044620513916\n",
            "Overall Loss 4.103509902954102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r3it [00:36, 12.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.842522144317627\n",
            "Dependecy Label Loss 3.880371570587158\n",
            "Overall Loss 4.120909214019775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r4it [00:48, 12.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.8447771072387695\n",
            "Dependecy Label Loss 3.8700318336486816\n",
            "Overall Loss 4.113718032836914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r5it [01:00, 12.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.807460308074951\n",
            "Dependecy Label Loss 3.8675732612609863\n",
            "Overall Loss 4.102545261383057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r6it [01:14, 12.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.816173553466797\n",
            "Dependecy Label Loss 3.8593785762786865\n",
            "Overall Loss 4.098577499389648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r7it [01:25, 12.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.823151111602783\n",
            "Dependecy Label Loss 3.85317063331604\n",
            "Overall Loss 4.09566593170166\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r8it [01:37, 12.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.797887802124023\n",
            "Dependecy Label Loss 3.8429136276245117\n",
            "Overall Loss 4.081657409667969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r9it [01:48, 11.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.8195481300354\n",
            "Dependecy Label Loss 3.8388705253601074\n",
            "Overall Loss 4.084039688110352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r10it [02:00, 11.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.786026477813721\n",
            "Dependecy Label Loss 3.839552164077759\n",
            "Overall Loss 4.076170921325684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r11it [02:12, 11.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.799475193023682\n",
            "Dependecy Label Loss 3.845923900604248\n",
            "Overall Loss 4.084311485290527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r12it [02:24, 11.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.827788352966309\n",
            "Dependecy Label Loss 3.822166681289673\n",
            "Overall Loss 4.073572158813477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "13it [02:38, 12.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relative Position Loss 4.800118923187256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a71490774983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mrel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_logits_pred_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Relative Position Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdep_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep_logits_pred_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdep_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Dependecy Label Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdep_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 49 is out of bounds."
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "lambdas = [0.25,0.5,0.75]\n",
        "distil_bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "for epoch_number in range(epochs):\n",
        "  model.train()\n",
        "  for _,batch in tqdm(enumerate(ud_generator[\"train\"])):\n",
        "    text_list,dep_tensors,rel_tensors = batch\n",
        "    \n",
        "    tokenized_val = [distil_bert_tokenizer(sentence, padding=True, return_tensors='pt').to(current_device) for sentence in text_list]\n",
        "    len_batch = []\n",
        "\n",
        "    # Getting length of maximum sentence\n",
        "    for token in tokenized_val: \n",
        "      len_batch.append(token['input_ids'].size()[1])\n",
        "\n",
        "    max_sentence_length = (max(len_batch))\n",
        "\n",
        "    dep_tensors = pad_list_of_tensors(dep_tensors,0,max_sentence_length)\n",
        "    rel_tensors = pad_list_of_tensors(rel_tensors,0,max_sentence_length)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    rel_logits_pred = []\n",
        "    dep_logits_pred = []\n",
        "    for sentence_idx in range(len(text_list)):\n",
        "      sentence = \"[CLS] \"+text_list[sentence_idx]+\" [SEP]\"\n",
        "      keep_tokens_idx = torch.tensor(custom_tokenize(distil_bert_tokenizer.tokenize(sentence))).to(current_device)\n",
        "\n",
        "      encoded_ip_tensor = distil_bert_tokenizer(sentence,return_tensors='pt',padding=True).to(current_device)\n",
        "      rel_logits,dep_logits = model(encoded_ip_tensor)\n",
        "\n",
        "\n",
        "      rel_logits_tensor = torch.index_select(rel_logits, 1, keep_tokens_idx)\n",
        "      dep_logits_tensor = torch.index_select(dep_logits, 1, keep_tokens_idx)\n",
        "\n",
        "      \n",
        "      rel_pad = torch.zeros(1, max_sentence_length - rel_logits_tensor.size()[1], 122).to(current_device)\n",
        "      rel_logits_tensor = torch.cat([rel_logits_tensor, rel_pad], dim=1)\n",
        "\n",
        "      dep_pad = torch.zeros(1, max_sentence_length - dep_logits_tensor.size()[1], 48).to(current_device)\n",
        "      dep_logits_tensor = torch.cat([dep_logits_tensor, dep_pad], dim=1)\n",
        "\n",
        "      rel_logits_pred.append(rel_logits_tensor)\n",
        "      dep_logits_pred.append(dep_logits_tensor)\n",
        "    \n",
        "\n",
        "    rel_logits_pred_tensor = torch.stack(rel_logits_pred, dim=1).squeeze(0).to(current_device)\n",
        "    dep_logits_pred_tensor = torch.stack(dep_logits_pred, dim=1).squeeze(0).to(current_device)\n",
        "    \n",
        "\n",
        "    rel_logits_pred_tensor = torch.reshape(rel_logits_pred_tensor,[32,122,max_sentence_length])\n",
        "    dep_logits_pred_tensor = torch.reshape(dep_logits_pred_tensor,[32,48,max_sentence_length])\n",
        "\n",
        "    rel_loss = rel_criterion(rel_logits_pred_tensor,rel_tensors)\n",
        "    print (\"Relative Position Loss\",rel_loss.item())\n",
        "    dep_loss = dep_criterion(dep_logits_pred_tensor,dep_tensors)\n",
        "    print (\"Dependecy Label Loss\",dep_loss.item())\n",
        "\n",
        "    overall_loss = lambdas[0]*rel_loss + (1-lambdas[0])*dep_loss\n",
        "    print (\"Overall Loss\",overall_loss.item())\n",
        "\n",
        "    overall_loss.backward()\n",
        "\n",
        "    # Have gradients at this point.\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)\n",
        "    optimizer.step()\n",
        "  \n",
        "  \n",
        "  model.eval()\n",
        "  for _,batch in tqdm(enumerate(ud_generator[\"valid\"])):\n",
        "    #do similar as in the loop above\n",
        "    text_list,dep_tensors,rel_tensors = batch\n",
        "    \n",
        "    tokenized_val = [distil_bert_tokenizer(sentence, padding=True, return_tensors='pt').to(current_device) for sentence in text_list]\n",
        "    len_batch = []\n",
        "\n",
        "    # Getting length of maximum sentence\n",
        "    for token in tokenized_val: \n",
        "      len_batch.append(token['input_ids'].size()[1])\n",
        "\n",
        "    max_sentence_length = (max(len_batch))\n",
        "\n",
        "    dep_tensors = pad_list_of_tensors(dep_tensors,0,max_sentence_length)\n",
        "    rel_tensors = pad_list_of_tensors(rel_tensors,0,max_sentence_length)\n",
        "    \n",
        "    rel_logits_pred = []\n",
        "    dep_logits_pred = []\n",
        "    for sentence_idx in range(len(text_list)):\n",
        "      sentence = \"[CLS] \"+text_list[sentence_idx]+\" [SEP]\"\n",
        "      keep_tokens_idx = torch.tensor(custom_tokenize(distil_bert_tokenizer.tokenize(sentence))).to(current_device)\n",
        "\n",
        "      encoded_ip_tensor = distil_bert_tokenizer(sentence,return_tensors='pt',padding=True).to(current_device)\n",
        "      rel_logits,dep_logits = model(encoded_ip_tensor)\n",
        "\n",
        "\n",
        "      rel_logits_tensor = torch.index_select(rel_logits, 1, keep_tokens_idx)\n",
        "      dep_logits_tensor = torch.index_select(dep_logits, 1, keep_tokens_idx)\n",
        "\n",
        "      print (\"Validation Rel Logits Argmax:\",rel_logits_tensor.argmax(-1))\n",
        "      print (\"Validation Rel Logits Argmax Shape:\",rel_logits_tensor.argmax(-1).shape)\n",
        "      print (\"Validation Dep Logits Argmax\",dep_logits_tensor.argmax(-1))\n",
        "      print (\"Validation Dep Logits Argmax Shape\",dep_logits_tensor.argmax(-1).shape)\n",
        "\n",
        "# Saving Model\n",
        "torch.save({\n",
        "    'options': options,\n",
        "    'model_dict': model.state_dict()\n",
        "    },'./saved_models/bert-parser-0.25.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
